{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6ed7b6a",
   "metadata": {},
   "source": [
    "## Herzlich Willkommen zu dem Packaging Valley Makeathon, willkommen bei Bosch Rexroth\n",
    "\n",
    "### Einführung in die Tools des Makeathons\n",
    "\n",
    "#### Jupyter Notebook\n",
    "**Ansprechpartner:** René Kübler\n",
    "\n",
    "Das Jupyter Notebook bietet eine interaktive Umgebung, um Python-Code auszuführen und diesen direkt mit erklärenden Textbausteinen zu kombinieren. Im Rahmen des Makeathons nutzen wir Jupyter Notebooks, um über eine API mit einem Large Language Model (LLM) wie in dem Fall Gemini Pro zu kommunizieren.\n",
    "\n",
    "Die folgenden Abschnitte erläutern die Funktionsweise der Codeblöcke im Notebook. Unser Ziel ist es, euch ein tieferes Verständnis für die implementierten Lösungen zu vermitteln. Neben diesen Erklärungen findet ihr im Notebook auch die konkreten Aufgabenstellungen sowie hilfreiche Hinweise. Bei weiterführenden Fragen stehen euch die genannten Ansprechpartner sowie das gesamte Team gerne zur Verfügung.\n",
    "\n",
    "\n",
    "#### ctrlX Works\n",
    "**Ansprechpartner:** Julian Schill, René Kübler\n",
    "\n",
    "ctrlX Works ist unsere Plattform zur Simulation der ctrlX, unserer eigens entwickelten Speicherprogrammierbaren Steuerung (SPS). Die ctrlX ist das Herzstück vieler Maschinen in unserer Modellfabrik, beispielsweise steuert sie flow6D. Während des Makeathons nutzen wir ctrlX Works, um euren entwickelten Code ausführlich zu testen, bevor er auf die reale ctrlX-Steuerung übertragen wird, die letztlich die Maschinen bedient.\n",
    "\n",
    "\n",
    "#### ctrlX FLOW6D\n",
    "**Ansprechpartner:** Julian Schill\n",
    "\n",
    "Das flow6D-System ist ein innovatives Transportsystem, das wir mit der ctrlX ansteuern werden. Es ermöglicht die Bewegung von Werkstücken auf sechs Achsen. Die zentrale Herausforderung und zugleich eure Kernaufgabe wird es sein, diese sogenannten „Mover“, die die Teile transportieren, anzusteuern über die ctrlX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d62b7e",
   "metadata": {},
   "source": [
    "### Der Code des Notebooks\n",
    "\n",
    "#### 1. Import der Bibliotheken\n",
    "\n",
    "Im ersten Codeblock werden die benötigten Bibliotheken `requests` und `json` importiert.\n",
    "\n",
    "*   **`requests`**: Diese Bibliothek ermöglicht es uns, HTTP-Anfragen zu stellen, beispielsweise um mit einer API zu kommunizieren.\n",
    "*   **`json`**: Diese Bibliothek dient der Arbeit mit JSON-Daten (JavaScript Object Notation). JSON ist ein weit verbreitetes, leichtgewichtiges Datenformat zum strukturierten Speichern und Übertragen von Daten. Es wird oft für Konfigurationsparameter oder als einfacher Output bei Tests verwendet, kann aber auch ganze Dateien umfassen, um komplexe Datenstrukturen abzubilden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424727e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
      "Requirement already satisfied: requests in c:\\users\\kur1ul\\onedrive - bosch group\\dokumente\\packageing_valley\\jupytertestnb\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\kur1ul\\onedrive - bosch group\\dokumente\\packageing_valley\\jupytertestnb\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kur1ul\\onedrive - bosch group\\dokumente\\packageing_valley\\jupytertestnb\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kur1ul\\onedrive - bosch group\\dokumente\\packageing_valley\\jupytertestnb\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kur1ul\\onedrive - bosch group\\dokumente\\packageing_valley\\jupytertestnb\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d8dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # For the http request\n",
    "import json # For the transfer of the parameters\n",
    "import time # Calculating the time from the web request"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce592071",
   "metadata": {},
   "source": [
    "#### 2. API-Konfiguration und Authentifizierung\n",
    "\n",
    "Dieser Codeblock definiert die grundlegenden Parameter für die Kommunikation mit dem Google Cloud Gateway, das den Zugang zum Large Language Model ermöglicht.\n",
    "\n",
    "*   **`API_GATEWAY_URL`**: Dies ist die Basis-URL des Google Cloud Gateways. Alle API-Anfragen an das Gemini Pro Modell werden an diese Adresse gesendet.\n",
    "*   **`GATEWAY_API_KEY`**: Dieser Schlüssel dient der Authentifizierung gegenüber dem Google Cloud Gateway selbst. Er identifiziert eure Anwendung als berechtigt, das Gateway zu nutzen.\n",
    "*   **`USER_API_KEY`**: Dieser persönliche Schlüssel identifiziert euch als spezifischen Nutzer. Er wird verwendet, um eure individuellen Anfragen zu verifizieren und zuzuordnen.\n",
    "\n",
    "Anschließend werden diese Schlüssel zusammen mit dem `Content-Type` in einem **`headers`**-Dictionary gebündelt. Dieses `headers`-Dictionary wird bei jeder API-Anfrage mitgesendet und stellt sicher, dass die Kommunikation korrekt formatiert und authentifiziert ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671165bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "API_GATEWAY_URL =  \"https://gemini-main-gateway-2zozxknb.ew.gateway.dev/\"\n",
    "\n",
    "\n",
    "GATEWAY_API_KEY = \"\"\n",
    "\n",
    "\n",
    "USER_API_KEY = \"\"\n",
    "\n",
    "prompt_text = \" Write a test Hello World\"\n",
    "\n",
    "timeout_seconds = 600 # hppt request timeout\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"x-api-key\": GATEWAY_API_KEY,\n",
    "    \"x-user-api-key\": USER_API_KEY,\n",
    "    \"LLM_parameters\": \"False\" # auf \"True\" setzen um die Parameter zu benutzen\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130bae02",
   "metadata": {},
   "source": [
    "#### 3. Nutzerprompt\n",
    "\n",
    "Im folgenden Abschnitt könnt ihr eure Anfrage an Gemini Pro eingeben. Dieser Prompt wird als Eingabe für das Large Language Model dienen, um die gewünschte Antwort zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04efef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt_text = \"test, respnd test\"\n",
    "\n",
    "prompt_text = ''' import time\n",
    "from flow6d import System,Mover\n",
    "\n",
    "def main():\n",
    "    \"\"\"Demonstrates the MoverSystem and Mover classes with dynamic discovery.\"\"\"\n",
    "    system = System()\n",
    "\n",
    "    available_mover_ids = system.list_available_movers()\n",
    "    print(f\"Available Movers: {available_mover_ids}\")\n",
    "\n",
    "    if not available_mover_ids:\n",
    "        print(\"No movers available. Exiting.\")\n",
    "        return\n",
    "\n",
    "    selected_mover_id = available_mover_ids[0]\n",
    "\n",
    "    if selected_mover_id not in available_mover_ids:\n",
    "        print(f\"Invalid mover ID selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    mover = system.get_mover(selected_mover_id)\n",
    "\n",
    "    if not mover:\n",
    "        print(f\"Could not retrieve mover {selected_mover_id}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    target_positions = [\n",
    "        (0.15, 0.2, 0.045, 0.0, 0.0, 0.0, 0.2, 0.1, 0.3, 0.2, 50.0, 50.0),\n",
    "        (0.15, 0.5, 0.05, 0.0, 0.0, 0.0, 0.2, 0.1, 0.3, 0.2, 50.0, 50.0),\n",
    "        (0.15, 0.2, 0.045, 0.0, 0.0, 0.0, 0.2, 0.1, 0.3, 0.2, 50.0, 50.0),\n",
    "        (0.15, 0.5, 0.05, 0.0, 0.0, 0.0, 0.2, 0.1, 0.3, 0.2, 50.0, 50.0)\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Controlling Mover {mover.mover_id} ---\")\n",
    "\n",
    "    for x, y, z, rx, ry, rz, lin_vel, rot_vel, lin_acc, rot_acc, lin_jerk, rot_jerk in target_positions:\n",
    "        mover.move_linear(\n",
    "            x, y, z, rx, ry, rz, linear_velocity=lin_vel,\n",
    "            rotation_velocity=rot_vel, linear_acceleration=lin_acc,\n",
    "            rotation_acceleration=rot_acc, linear_jerk=lin_jerk, rotation_jerk=rot_jerk\n",
    "        )\n",
    "\n",
    "        while True:\n",
    "            state = mover.get_moverstate()\n",
    "            if state == \"Holding\":\n",
    "                break\n",
    "            current_position = mover.get_position()\n",
    "            if current_position:\n",
    "                print(\n",
    "                    f\"Current Position: x={current_position.get('x')}, \"\n",
    "                    f\"y={current_position.get('y')}, z={current_position.get('z')}\"\n",
    "                )\n",
    "            time.sleep(0.1)            \n",
    "        print(\"point reached\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "schreibe mir das programm so um das ich einen Mover von postion 1 zu position 2 benege, benutze das für das Beispiel siehe oben\n",
    "\n",
    "    \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5b355",
   "metadata": {},
   "source": [
    "## 4. Konfiguration der API-Anfrage (Payload)\n",
    "\n",
    "Dieser Codeblock definiert den `payload` (die \"Nutzlast\"), der die tatsächlichen Daten und Einstellungen für die Anfrage an das Gemini Pro Modell enthält.\n",
    "\n",
    "*   **`prompt`**: Dies ist der eigentliche Text, den ihr als Anweisung oder Frage an das Large Language Model senden möchtet. Hier wird der zuvor definierte `prompt_text` eingesetzt.\n",
    "*   **`temperature`**: Steuert die Kreativität und Zufälligkeit der generierten Antwort.\n",
    "    *   Ein Wert nahe 0 (z.B. 0.0) führt zu sehr deterministischen und konservativen Antworten (weniger Variation).\n",
    "    *   Ein höherer Wert (z.B. 1.0) führt zu kreativeren und diverseren Antworten, die aber auch weniger vorhersehbar sein können.\n",
    "    *   **Standardempfehlung:** Für die meisten Anwendungsfälle liegt ein guter Startwert oft zwischen 0.5 und 0.9.\n",
    "*   **`top_p`**: Auch bekannt als \"nucleus sampling\". Dieser Parameter begrenzt die Auswahl der nächsten Wörter auf eine bestimmte kumulative Wahrscheinlichkeitsverteilung.\n",
    "    *   Ein Wert von 1.0 (Standard) bedeutet, dass alle Wörter berücksichtigt werden.\n",
    "    *   Ein Wert von 0.9 würde beispielsweise bedeuten, dass nur die Top-Wörter berücksichtigt werden, deren kumulierte Wahrscheinlichkeit 90% erreicht. Dies kann helfen, weniger plausible Wörter zu vermeiden und die Kohärenz zu verbessern.\n",
    "*   **`top_k`**: Limitiert die Auswahl der nächsten Wörter auf die `k` wahrscheinlichsten Optionen.\n",
    "    *   Ein Wert von 1 würde nur das wahrscheinlichste Wort auswählen.\n",
    "    *   Ein höherer Wert (z.B. 32) erweitert die Auswahl auf die 32 wahrscheinlichsten Wörter.\n",
    "    *   Dies kann dazu beitragen, die Vielfalt der generierten Antworten zu erhöhen, während gleichzeitig unpassende Wörter ausgeschlossen werden.\n",
    "*   **`candidate_count`**: Gibt an, wie viele verschiedene Antwortvorschläge (Kandidaten) das Modell generieren soll.\n",
    "    *   Ein Wert von 1 (Standard) bedeutet, dass nur eine Antwort generiert wird.\n",
    "    *   Höhere Werte können nützlich sein, um verschiedene Formulierungen oder Ansätze des Modells zu vergleichen.\n",
    "*   **`max_output_tokens`**: Definiert die maximale Länge der vom Modell generierten Antwort, gemessen in Tokens. Ein Token kann ein Wort, ein Teil eines Wortes oder ein Satzzeichen sein.\n",
    "    *   Ein höherer Wert erlaubt längere Antworten, kann aber auch die Rechenzeit erhöhen.\n",
    "    *   Stellt sicher, dass das Modell genügend \"Platz\" hat, um eine vollständige Antwort zu generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c00eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"prompt\": prompt_text,\n",
    "    \"temperature\"       : 0.9,\n",
    "    \"top_p\"             : 1.0,\n",
    "    \"top_k\"             : 32,\n",
    "    \"candidate_count\"   : 1,\n",
    "    \"max_output_tokens\" : 100000\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76888d27",
   "metadata": {},
   "source": [
    "#### 5. Ausführen der API-Anfrage und Fehlerbehandlung\n",
    "\n",
    "Dieser Codeblock ist für das Senden der konfigurierten Anfrage an die Gemini Pro API und für die robuste Behandlung potenzieller Fehler zuständig.\n",
    "\n",
    "Der gesamte Vorgang ist in einem `try...except`-Block gekapselt, um auf mögliche Probleme während der Kommunikation reagieren zu können.\n",
    "\n",
    "*   **`try` Block**:\n",
    "    *   **`response = requests.post(API_GATEWAY_URL, headers=headers, data=json.dumps(payload))`**: Hier wird die eigentliche POST-Anfrage gesendet.\n",
    "        *   `API_GATEWAY_URL`: Die zuvor definierte URL des API-Gateways.\n",
    "        *   `headers`: Das Dictionary mit den Authentifizierungs-Headern.\n",
    "        *   `data=json.dumps(payload)`: Der konfigurierte `payload` (Nutzer-Prompt und Parameter) wird in ein JSON-Format umgewandelt und als Datenkörper der Anfrage mitgeschickt.\n",
    "    *   **`response.raise_for_status()`**: Diese Methode prüft den Statuscode der Serverantwort. Wenn der Statuscode auf einen Client-Fehler (4xx) oder Server-Fehler (5xx) hinweist, wird automatisch eine `requests.exceptions.HTTPError` Ausnahme ausgelöst. Dies vereinfacht die Fehlererkennung.\n",
    "    *   **`print(\"STATUSCODE:\", response.status_code)`**: Gibt den HTTP-Statuscode der erfolgreichen Antwort aus (erwartet wird `200` für \"OK\").\n",
    "    *   **`print(response.text)`**: Zeigt den eigentlichen Textinhalt der Serverantwort an, welche die generierte Antwort von Gemini Pro enthalten sollte.\n",
    "\n",
    "*   **`except requests.exceptions.HTTPError as e` Block**:\n",
    "    *   Dieser Block wird ausgeführt, wenn die `response.raise_for_status()`-Methode einen HTTP-Fehler erkannt hat (z.B. falsche Authentifizierung, Serverfehler).\n",
    "    *   Er gibt detaillierte Informationen über den aufgetretenen Fehler aus, einschließlich des Statuscodes und der Fehlermeldung vom Server, um die Problembehebung zu erleichtern.\n",
    "\n",
    "*   **`except requests.exceptions.RequestException as e` Block**:\n",
    "    *   Dieser Block fängt alle anderen Arten von `requests`-spezifischen Fehlern ab, die nicht direkt HTTP-Fehler sind. Dazu gehören beispielsweise Probleme mit der Netzwerkverbindung, DNS-Fehler oder Timeouts.\n",
    "    *   Er informiert den Benutzer über einen allgemeinen Verbindungsfehler.\n",
    "\n",
    "Hinweise: \n",
    "\n",
    "- Diese robuste Fehlerbehandlung ist entscheidend, um die Stabilität der Anwendung zu gewährleisten und bei Problemen nützliche Rückmeldungen zu liefern.\n",
    "- In der Cloud wird dein Prompt noch durch folgenden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b133cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- EIN FEHLER IST AUFGETRETEN ---\n",
      "Statuscode: 504\n",
      "Antwort vom Server: {\"code\":504,\"message\":\"upstream request timeout\"}\n",
      "\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start_time = time.time()\n",
    "    # make a post request\n",
    "    response = requests.post(\n",
    "                            API_GATEWAY_URL, \n",
    "                            headers=headers, \n",
    "                            data=json.dumps(payload),\n",
    "                            timeout= timeout_seconds,\n",
    "                            )\n",
    "\n",
    "    # look for a 4xx 5xx error)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    \n",
    "    print(\"\\n--- ERFOLG! ---\")\n",
    "    print(\"STATUSCODE:\", response.status_code) # should send 200\n",
    "    print(\"\\nAntwort vom Gemini-Modell:\")\n",
    "    print(response.text)\n",
    "    print(\"------------------------------------------\")\n",
    "    print(\"\\nThe system is WOrking successfully\")\n",
    "    end_time = time.time()\n",
    "    time_dif = end_time - start_time\n",
    "    print(f\"time for response: {time_dif}\")\n",
    "\n",
    "except requests.exceptions.HTTPError as e:\n",
    "    # takes care of Errors linke 404, 403, 500\n",
    "    print(\"\\n--- An error occurred  ---\")\n",
    "    print(f\"Statuscode: {e.response.status_code}\")\n",
    "    print(f\"Server Message: {e.response.text}\")\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # takess care of network issues\n",
    "    print(f\"\\nAn connection Error occurred  {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56224870",
   "metadata": {},
   "source": [
    "#### Aufgabenebschreibung\n",
    "1.1 Schreibe einen Prompt, der den Mover von Position 1 zu Position 2 fahren lässt. Dabei sollen die Mover mit den folgenden Parametern angesteuert werden:\n",
    "- Position x\n",
    "- Position y\n",
    "- Geschwindigkeit\n",
    "\n",
    "1.2 Erweitere den Prompt so, dass die folgenden Parameter auch angesteuert werden:\n",
    "- Wippen\n",
    "- Drehen\n",
    "\n",
    "2.1 Baue die Anfrage so um, dass du einen Nutzerprompt hast und einen Systemprompt. Dabei unterscheiden sich die beiden insofern, dass der Nutzerprompt sich bei jeder Anfrage verändert und der Systemprompt jedes Mal gleich bleibt. Dabei spiegelt der Nutzerprompt die Intention des Nutzers wider und der Systemprompt reguliert die Nutzerausgabe.\n",
    "\n",
    "2.2 Schreibe einen einfachen Satz in die Nutzerausgabe, wie \"Ich will, dass der Mover von hier (x,y) zu (x,y) fährt und sich an Position (x,y) dreht\". Schreibe den Systemprompt so, dass er eine Anfrage händeln kann, die z. B. unvollständig oder sehr unkonkret ist.\n",
    "\n",
    "2.3 Ändere deinen Systemprompt so, dass das System in der Lage ist, mit unterschiedlichen Hindernissen umzugehen und diese zu umfahren.\n",
    "\n",
    "2.4 Szenario: Ein Mover wartet und steht dabei einem anderem im Weg. Erweitere deinen Systemprompt so, dass der Mover 1 zu dem Mover 2 fährt, wartet, bis dieser seine Position verlassen hat, um Platz zu machen, und fahre im Anschluss den Mover 1 an die Position von Mover 1.\n",
    "\n",
    "3.1 Es gibt einen Hindernis-Parcours. Wie einfach kann der User-Prompt sein, sodass der Mover den Parcours durchläuft?\n",
    "\n",
    "3.2 Lass zwei Mover mit einander inteagieren, so das beide sich parallel bewegen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46051f29",
   "metadata": {},
   "source": [
    "#### Prompting-Techniken und Probleme bei der Codegenerierung\n",
    "\n",
    "Bei der Nutzung von Large Language Models (LLMs) zur Codegenerierung ist die Qualität des Prompts entscheidend für das Ergebnis. Ein gut formulierter Prompt kann zu präzisem, lauffähigem und optimiertem Code führen, während ein unklarer oder unvollständiger Prompt zu irrelevanten, fehlerhaften oder sogar unsicheren Ergebnissen führen kann.\n",
    "\n",
    "##### Häufige Prompting-Techniken für Codegenerierung:\n",
    "\n",
    "1.  **Zero-Shot Prompting:**\n",
    "    *   **Beschreibung:** Eine direkte Anweisung, ohne Beispiele. Das LLM muss die Aufgabe aus dem Kontext und seinem Training selbstständig lösen.\n",
    "    *   **Beispiel:** \"Schreibe eine Python-Funktion, die zwei Zahlen addiert.\"\n",
    "    *   **Anwendung:** Gut für einfache, klar definierte Aufgaben.\n",
    "\n",
    "2.  **Few-Shot Prompting:**\n",
    "    *   **Beschreibung:** Der Prompt enthält ein oder mehrere Beispiele für die gewünschte Eingabe und Ausgabe, um das LLM auf das gewünschte Format oder den Stil einzustimmen.\n",
    "    *   **Beispiel:**\n",
    "        \n",
    "      Beispiel 1:\n",
    "      Input: Eine Funktion, die \"Hallo Welt\" ausgibt. <br>\n",
    "      Output: <br>\n",
    "      def greet_world(): <br>\n",
    "      <div style=\"margin-left: 2em;\">print(\"Hallo Welt\")\n",
    "      </div>\n",
    "      <br> Beispiel 2:\n",
    "      Input: Eine Funktion, die zwei Zahlen addiert. <br>\n",
    "      Output: <br>\n",
    "      def add_numbers(a, b): <br>\n",
    "      <div style=\"margin-left: 2em;\"> return a + b </div> <br>\n",
    "\n",
    "      Input: Eine Funktion, die den größten Wert in einer Liste findet.\n",
    "      Output:\n",
    "     \n",
    "        \n",
    "    *   **Anwendung:** Ideal, um das LLM an spezifische Coding-Stile, komplexe Logiken oder die Einhaltung bestimmter API-Signaturen zu gewöhnen.\n",
    "\n",
    "3.  **Chain-of-Thought (CoT) Prompting:**\n",
    "    *   **Beschreibung:** Das LLM wird angewiesen, seine Denkprozesse oder Zwischenschritte zu verbalisieren, bevor es die endgültige Antwort (den Code) generiert. Dies hilft dem Modell, komplexe Probleme schrittweise zu lösen.\n",
    "    *   **Beispiel:** \"Schreibe eine Python-Funktion, die prüft, ob eine Zahl prim ist. Denke Schritt für Schritt darüber nach, wie eine Primzahl definiert ist und welche Prüfungen notwendig sind, bevor du den Code schreibst.\"\n",
    "    *   **Anwendung:** Besonders nützlich für algorithmenlastige oder mehrstufige Codierungsaufgaben.\n",
    "\n",
    "4.  **Role-Playing / Persona Prompting:**\n",
    "    *   **Beschreibung:** Das LLM nimmt die Rolle eines bestimmten Experten an (z.B. ein erfahrener Python-Entwickler, ein Sicherheitsexperte).\n",
    "    *   **Beispiel:** \"Du bist ein erfahrener JavaScript-Entwickler. Schreibe eine asynchrone Funktion, die Daten von einer API abruft und Fehler behandelt.\"\n",
    "    *   **Anwendung:** Beeinflusst den Stil, die Best Practices und die Gründlichkeit des generierten Codes.\n",
    "\n",
    "##### Häufige Probleme bei der Codegenerierung durch LLMs:\n",
    "\n",
    "1.  **\"Lost in the Middle\" (Verlust im Kontext):**\n",
    "    *   **Problem:** LLMs tendieren dazu, Informationen am Anfang und Ende eines längeren Prompts besser zu verarbeiten als jene in der Mitte. Bei umfangreichen Code-Anfragen oder detaillierten Spezifikationen in der Mitte des Prompts kann das LLM wichtige Details übersehen.\n",
    "    *   **Lösung:** Schlüsselinformationen oder entscheidende Randbedingungen am Anfang oder Ende des Prompts platzieren. Prompts in kleinere, fokussierte Schritte unterteilen.\n",
    "\n",
    "2.  **Unzureichende oder Vage Spezifikationen:**\n",
    "    *   **Problem:** Wenn der Prompt zu allgemein gehalten ist (z.B. \"Schreibe eine Web-App\"), fehlt dem LLM die notwendige Klarheit über die gewünschte Funktionalität, Technologie (Frameworks, Sprachen), Architektur oder Fehlerbehandlung.\n",
    "    *   **Lösung:** So präzise wie möglich sein: Gebt die gewünschte Programmiersprache, Frameworks, erwartete Ein- und Ausgaben, Fehlerbehandlungsstrategien und Testfälle an.\n",
    "\n",
    "3.  **Halluzinationen / Faktische Fehler im Code:**\n",
    "    *   **Problem:** Das LLM kann Code generieren, der syntaktisch korrekt aussieht, aber logische Fehler enthält, falsche APIs verwendet oder Best Practices ignoriert. Dies ist besonders kritisch bei sicherheitsrelevantem Code.\n",
    "    *   **Lösung:** Prompting mit expliziten Anforderungen an Robustheit, Effizienz oder Sicherheitsaspekte. Immer den generierten Code *gründlich überprüfen und testen*, da LLMs keine Garantien für Korrektheit bieten.\n",
    "\n",
    "4.  **Fehlende Kontextkenntnisse (Domänenwissen):**\n",
    "    *   **Problem:** LLMs haben kein intrinsisches Verständnis für eure spezifische Codebasis, interne APIs, Unternehmensstandards oder komplexe Domänenlogiken. Der generierte Code passt möglicherweise nicht nahtlos in eure bestehende Infrastruktur.\n",
    "    *   **Lösung:** Relevante Code-Snippets, API-Dokumentationen oder interne Richtlinien im Prompt bereitstellen (Few-Shot). Fine-Tuning des Modells mit eigenen Daten (fortgeschritten).\n",
    "\n",
    "5.  **Formatierungs- und Layout-Probleme:**\n",
    "    *   **Problem:** Obwohl LLMs Code generieren können, halten sie sich nicht immer an spezifische Code-Formatierungsstandards (PEP8 für Python, etc.), Indentierungskonventionen oder bevorzugte Kommentierungsstile.\n",
    "    *   **Lösung:** Im Prompt explizit Formatierungsregeln (z.B. \"halte dich an PEP8\", \"verwende 4 Leerzeichen für Einrückungen\") und Kommentierungsstile vorgeben. Beispiele (Few-Shot) für den gewünschten Stil sind hier sehr effektiv.\n",
    "\n",
    "Durch das Verständnis dieser Techniken und potenziellen Fallstricke könnt ihr die Effektivität von LLMs bei der Codegenerierung erheblich steigern und bessere, zuverlässigere Ergebnisse erzielen.#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86213d0f",
   "metadata": {},
   "source": [
    "#### Inforamtionen zum Google Cloud Backend\n",
    "\n",
    "Informationen zum Backend: https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro?hl=de\n",
    "\n",
    "Für weitere Informationen wende dich gerne an Rene Kübler\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
